import pandas as pd

import pandas as pd

def summarize_adjustments(df, amount_col='adj_amount', desc_col='adj_description', ecl_col='ecl_flag'):
    """
    Summarize adjustments based on ECL flag logic:
      - If ecl_flag == True â†’ take positive amounts
      - If ecl_flag == False â†’ take negative amounts
    Uses absolute values to reflect magnitude.
    Shows absolute adjustment amounts in the output.
    Adds '%' sign to percentage columns.
    """
    df = df.copy()
    
    # Keep only valid descriptions
    df = df[df[desc_col].notna() & (df[desc_col].astype(str).str.strip() != '')]
    
    # Filter based on ECL flag
    df_filtered = df[
        ((df[ecl_col] == True) & (df[amount_col] > 0)) |
        ((df[ecl_col] == False) & (df[amount_col] < 0))
    ].copy()
    
    if df_filtered.empty:
        return pd.DataFrame(columns=[
            desc_col, 'abs_amount', 'adjusted_amount', 'pct_adjusted_amount',
            'adjusted_count', 'pct_adjusted_count'
        ])
    
    # Use absolute values for magnitude-based sums
    df_filtered['_abs_amt'] = df_filtered[amount_col].abs()
    
    # Totals
    total_amt = df_filtered['_abs_amt'].sum()
    total_count = len(df_filtered)
    
    # Group by adjustment description
    summary = (
        df_filtered.groupby(desc_col)
        .agg(
            abs_amount=('_abs_amt', 'sum'),
            adjusted_amount=('_abs_amt', 'sum'),  # same as abs_amount (for clarity)
            adjusted_count=(amount_col, 'count')
        )
        .reset_index()
    )
    
    # Compute percentages
    summary['pct_adjusted_amount'] = (
        (summary['adjusted_amount'] / total_amt * 100)
        .round(2).astype(str) + '%'
    )
    summary['pct_adjusted_count'] = (
        (summary['adjusted_count'] / total_count * 100)
        .round(2).astype(str) + '%'
    )
    
    # Sort by magnitude descending
    summary = summary.sort_values('abs_amount', ascending=False)
    
    return summary


from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def cluster_adjustments(df, amount_col='adj_amount',
                        desc_col='adj_description',
                        custom_col='changed_customs',
                        custom_val_col='changed_custom_values',
                        max_k=10, random_state=42):
    """
    Cluster negative adjustment entries based on numeric and categorical similarity.
    """

    # --- Filter negatives only
    df_neg = df[df[amount_col] < 0].copy()
    df_neg = df_neg[[amount_col, desc_col, custom_col, custom_val_col]].dropna(subset=[amount_col])

    if df_neg.empty:
        print("No negative adjustments found.")
        return None

    # --- Fill NaNs with placeholder for categorical encoding
    for col in [desc_col, custom_col, custom_val_col]:
        df_neg[col] = df_neg[col].fillna("None").astype(str)

    # --- One-hot encode categorical columns
    ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')
    cat_features = ohe.fit_transform(df_neg[[desc_col, custom_col, custom_val_col]])
    cat_cols = ohe.get_feature_names_out([desc_col, custom_col, custom_val_col])
    cat_df = pd.DataFrame(cat_features, columns=cat_cols, index=df_neg.index)

    # --- Combine with numerical column
    X = pd.concat([df_neg[[amount_col]].reset_index(drop=True), cat_df.reset_index(drop=True)], axis=1)

    # --- Standardize numerical features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # --- Find optimal number of clusters (Elbow + Silhouette)
    distortions, silhouettes = [], []
    K = range(2, max_k + 1)

    for k in K:
        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)
        kmeans.fit(X_scaled)
        distortions.append(kmeans.inertia_)
        sil = silhouette_score(X_scaled, kmeans.labels_)
        silhouettes.append(sil)

    # --- Plot elbow and silhouette
    fig, ax1 = plt.subplots(1, 2, figsize=(12, 5))
    ax1[0].plot(K, distortions, marker='o')
    ax1[0].set_title('Elbow Method (Inertia)')
    ax1[0].set_xlabel('Number of clusters (k)')
    ax1[0].set_ylabel('Inertia')

    ax1[1].plot(K, silhouettes, marker='o')
    ax1[1].set_title('Silhouette Score')
    ax1[1].set_xlabel('Number of clusters (k)')
    ax1[1].set_ylabel('Score')
    plt.tight_layout()
    plt.show()

    # --- Choose optimal k (highest silhouette)
    best_k = K[np.argmax(silhouettes)]
    best_silhouette = np.max(silhouettes)
    print(f"âœ… Optimal clusters: {best_k} | Silhouette score: {best_silhouette:.3f}")

    # --- Final clustering
    kmeans_final = KMeans(n_clusters=best_k, random_state=random_state, n_init=10)
    df_neg['cluster'] = kmeans_final.fit_predict(X_scaled)

    # --- Cluster size summary
    cluster_summary = df_neg.groupby('cluster').agg(
        count=('cluster', 'size'),
        avg_amount=(amount_col, 'mean')
    ).reset_index()

    print("\nðŸ“Š Cluster Summary:")
    print(cluster_summary)

    return df_neg, cluster_summary, best_silhouette

import pandas as pd

def summarize_clusters(df, cluster_col='cluster',
                       desc_col='adj_description',
                       custom_col='changed_customs',
                       custom_val_col='changed_custom_values'):
    """
    Summarize clustered adjustment data.
    
    Groups by cluster and aggregates unique values of description,
    changed_customs, and changed_custom_values into comma-separated strings.
    """
    if cluster_col not in df.columns:
        raise ValueError(f"Column '{cluster_col}' not found in DataFrame.")

    summary = (
        df.groupby(cluster_col)
          .agg({
              desc_col: lambda x: ', '.join(sorted(set([str(i) for i in x if pd.notna(i)]))),
              custom_col: lambda x: ', '.join(sorted(set([str(i) for i in x if pd.notna(i)]))),
              custom_val_col: lambda x: ', '.join(sorted(set([str(i) for i in x if pd.notna(i)]))),
              'adj_amount': ['count', 'mean', 'sum']
          })
          .reset_index()
    )

    # Flatten column names
    summary.columns = ['cluster', 'adj_descriptions', 'changed_customs',
                       'changed_custom_values', 'count', 'mean_adj', 'sum_adj']

    # Sort clusters by total magnitude of adjustment
    summary = summary.sort_values(by='sum_adj', ascending=True).reset_index(drop=True)

    return summary


import pandas as pd

def save_dfs_to_excel(dfs, sheet_names=None, file_name='output.xlsx'):
    """
    Saves a list of DataFrames to an Excel file, each on a separate sheet (no index).

    Parameters
    ----------
    dfs : list[pd.DataFrame]
        List of DataFrames to save.
    sheet_names : list[str], optional
        Names of sheets. If None, they will be auto-named as 'Sheet1', 'Sheet2', etc.
    file_name : str
        Output Excel file name.
    """
    if sheet_names is None:
        sheet_names = [f"Sheet{i+1}" for i in range(len(dfs))]
    elif len(sheet_names) != len(dfs):
        raise ValueError("Length of sheet_names must match number of DataFrames.")
    
    with pd.ExcelWriter(file_name, engine='openpyxl') as writer:
        for df, name in zip(dfs, sheet_names):
            df.to_excel(writer, sheet_name=name, index=False)
    
    print(f"âœ… Excel file '{file_name}' created with {len(dfs)} sheets.")


import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from scipy.sparse import hstack
import matplotlib.pyplot as plt

def hybrid_clustering(df, amount_col='adj_amount',
                      desc_col='adj_description',
                      custom_col='changed_customs',
                      custom_val_col='changed_custom_values',
                      max_k=10, eps=0.5, min_samples=3):
    """
    Performs three clustering methods:
      1. KMeans on numerical + one-hot features
      2. KMeans on TF-IDF text embeddings
      3. DBSCAN clustering

    Returns: DataFrame with cluster assignments and prints elbow & silhouette analysis.
    """

    df = df.copy()
    df = df.dropna(subset=[desc_col])  # remove blank descriptions

    # ---------- Step 1: One-hot encode categorical columns ----------
    cat_features = df[[desc_col, custom_col, custom_val_col]].astype(str).fillna('None')
    ohe = OneHotEncoder(sparse=True, handle_unknown='ignore')
    X_cat = ohe.fit_transform(cat_features)

    # Numeric scaling for adj_amount
    X_num = StandardScaler().fit_transform(df[[amount_col]])

    # Combine numeric + categorical
    X_combined = hstack([X_num, X_cat])

    # ---------- Step 2: TF-IDF features ----------
    tfidf = TfidfVectorizer(stop_words='english', max_features=1000)
    tfidf_matrix = tfidf.fit_transform(
        df[desc_col].astype(str) + " " + df[custom_col].astype(str) + " " + df[custom_val_col].astype(str)
    )

    # ---------- Step 3: Elbow method for KMeans ----------
    inertia = []
    K = range(2, max_k + 1)
    for k in K:
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        km.fit(X_combined)
        inertia.append(km.inertia_)

    plt.figure(figsize=(5, 3))
    plt.plot(K, inertia, 'bx-')
    plt.title('Elbow Method (KMeans on OHE)')
    plt.xlabel('k')
    plt.ylabel('Inertia')
    plt.show()

    # Optimal cluster selection
    optimal_k = np.diff(inertia, 2).argmin() + 2

    # ---------- Step 4: Silhouette score (numerical + OHE) ----------
    km_combined = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    labels_combined = km_combined.fit_predict(X_combined)
    sil_combined = silhouette_score(X_combined, labels_combined)

    print(f"âœ… KMeans (OHE) Optimal Clusters = {optimal_k} | Silhouette = {sil_combined:.3f}")

    # ---------- Step 5: TF-IDF KMeans ----------
    inertia_tfidf = []
    for k in K:
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        km.fit(tfidf_matrix)
        inertia_tfidf.append(km.inertia_)

    plt.figure(figsize=(5, 3))
    plt.plot(K, inertia_tfidf, 'gx-')
    plt.title('Elbow Method (KMeans on TF-IDF)')
    plt.xlabel('k')
    plt.ylabel('Inertia')
    plt.show()

    optimal_k_tfidf = np.diff(inertia_tfidf, 2).argmin() + 2
    km_tfidf = KMeans(n_clusters=optimal_k_tfidf, random_state=42, n_init=10)
    labels_tfidf = km_tfidf.fit_predict(tfidf_matrix)
    sil_tfidf = silhouette_score(tfidf_matrix, labels_tfidf)

    print(f"âœ… KMeans (TF-IDF) Optimal Clusters = {optimal_k_tfidf} | Silhouette = {sil_tfidf:.3f}")

    # ---------- Step 6: DBSCAN ----------
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    labels_dbscan = dbscan.fit_predict(X_combined)
    n_clusters_dbscan = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)
    if n_clusters_dbscan > 1:
        sil_dbscan = silhouette_score(X_combined, labels_dbscan)
    else:
        sil_dbscan = np.nan

    print(f"âœ… DBSCAN Clusters = {n_clusters_dbscan} | Silhouette = {sil_dbscan}")

    # ---------- Step 7: Merge results ----------
    df['kmeans_cluster'] = labels_combined
    df['tfidf_cluster'] = labels_tfidf
    df['dbscan_cluster'] = labels_dbscan

    return df, {
        'kmeans_clusters': optimal_k,
        'tfidf_clusters': optimal_k_tfidf,
        'dbscan_clusters': n_clusters_dbscan,
        'silhouette_kmeans': sil_combined,
        'silhouette_tfidf': sil_tfidf,
        'silhouette_dbscan': sil_dbscan
    }

