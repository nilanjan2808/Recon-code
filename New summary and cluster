import pandas as pd

def summarize_adjustments(df, amount_col='adj_amount', desc_col='adj_description'):
    # Filter negative adjustments
    df_neg = df[df[amount_col] < 0].copy()
    
    if df_neg.empty:
        return pd.DataFrame(columns=[
            desc_col, 'sum_negative_amount', 'pct_negative_amount', 
            'count_negative', 'pct_negative_count'
        ])
    
    # Compute total stats
    total_neg_amt = df_neg[amount_col].sum()
    total_count = len(df_neg)
    
    # Group by adjustment description
    summary = (
        df_neg.groupby(desc_col)
        .agg(sum_negative_amount=(amount_col, 'sum'),
             count_negative=(amount_col, 'count'))
        .reset_index()
    )
    
    # Compute percentages
    summary['pct_negative_amount'] = (
        summary['sum_negative_amount'] / total_neg_amt * 100
    ).round(2)
    summary['pct_negative_count'] = (
        summary['count_negative'] / total_count * 100
    ).round(2)
    
    # Sort by sum of negatives (descending)
    summary = summary.sort_values('sum_negative_amount', ascending=True)
    
    return summary

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def cluster_adjustments(df, amount_col='adj_amount',
                        desc_col='adj_description',
                        custom_col='changed_customs',
                        custom_val_col='changed_custom_values',
                        max_k=10, random_state=42):
    """
    Cluster negative adjustment entries based on numeric and categorical similarity.
    """

    # --- Filter negatives only
    df_neg = df[df[amount_col] < 0].copy()
    df_neg = df_neg[[amount_col, desc_col, custom_col, custom_val_col]].dropna(subset=[amount_col])

    if df_neg.empty:
        print("No negative adjustments found.")
        return None

    # --- Fill NaNs with placeholder for categorical encoding
    for col in [desc_col, custom_col, custom_val_col]:
        df_neg[col] = df_neg[col].fillna("None").astype(str)

    # --- One-hot encode categorical columns
    ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')
    cat_features = ohe.fit_transform(df_neg[[desc_col, custom_col, custom_val_col]])
    cat_cols = ohe.get_feature_names_out([desc_col, custom_col, custom_val_col])
    cat_df = pd.DataFrame(cat_features, columns=cat_cols, index=df_neg.index)

    # --- Combine with numerical column
    X = pd.concat([df_neg[[amount_col]].reset_index(drop=True), cat_df.reset_index(drop=True)], axis=1)

    # --- Standardize numerical features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # --- Find optimal number of clusters (Elbow + Silhouette)
    distortions, silhouettes = [], []
    K = range(2, max_k + 1)

    for k in K:
        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)
        kmeans.fit(X_scaled)
        distortions.append(kmeans.inertia_)
        sil = silhouette_score(X_scaled, kmeans.labels_)
        silhouettes.append(sil)

    # --- Plot elbow and silhouette
    fig, ax1 = plt.subplots(1, 2, figsize=(12, 5))
    ax1[0].plot(K, distortions, marker='o')
    ax1[0].set_title('Elbow Method (Inertia)')
    ax1[0].set_xlabel('Number of clusters (k)')
    ax1[0].set_ylabel('Inertia')

    ax1[1].plot(K, silhouettes, marker='o')
    ax1[1].set_title('Silhouette Score')
    ax1[1].set_xlabel('Number of clusters (k)')
    ax1[1].set_ylabel('Score')
    plt.tight_layout()
    plt.show()

    # --- Choose optimal k (highest silhouette)
    best_k = K[np.argmax(silhouettes)]
    best_silhouette = np.max(silhouettes)
    print(f"âœ… Optimal clusters: {best_k} | Silhouette score: {best_silhouette:.3f}")

    # --- Final clustering
    kmeans_final = KMeans(n_clusters=best_k, random_state=random_state, n_init=10)
    df_neg['cluster'] = kmeans_final.fit_predict(X_scaled)

    # --- Cluster size summary
    cluster_summary = df_neg.groupby('cluster').agg(
        count=('cluster', 'size'),
        avg_amount=(amount_col, 'mean')
    ).reset_index()

    print("\nðŸ“Š Cluster Summary:")
    print(cluster_summary)

    return df_neg, cluster_summary, best_silhouette

import pandas as pd

def summarize_clusters(df, cluster_col='cluster',
                       desc_col='adj_description',
                       custom_col='changed_customs',
                       custom_val_col='changed_custom_values'):
    """
    Summarize clustered adjustment data.
    
    Groups by cluster and aggregates unique values of description,
    changed_customs, and changed_custom_values into comma-separated strings.
    """
    if cluster_col not in df.columns:
        raise ValueError(f"Column '{cluster_col}' not found in DataFrame.")

    summary = (
        df.groupby(cluster_col)
          .agg({
              desc_col: lambda x: ', '.join(sorted(set([str(i) for i in x if pd.notna(i)]))),
              custom_col: lambda x: ', '.join(sorted(set([str(i) for i in x if pd.notna(i)]))),
              custom_val_col: lambda x: ', '.join(sorted(set([str(i) for i in x if pd.notna(i)]))),
              'adj_amount': ['count', 'mean', 'sum']
          })
          .reset_index()
    )

    # Flatten column names
    summary.columns = ['cluster', 'adj_descriptions', 'changed_customs',
                       'changed_custom_values', 'count', 'mean_adj', 'sum_adj']

    # Sort clusters by total magnitude of adjustment
    summary = summary.sort_values(by='sum_adj', ascending=True).reset_index(drop=True)

    return summary


import pandas as pd

def save_dfs_to_excel(dfs, sheet_names=None, file_name='output.xlsx'):
    """
    Saves a list of DataFrames to an Excel file, each on a separate sheet (no index).

    Parameters
    ----------
    dfs : list[pd.DataFrame]
        List of DataFrames to save.
    sheet_names : list[str], optional
        Names of sheets. If None, they will be auto-named as 'Sheet1', 'Sheet2', etc.
    file_name : str
        Output Excel file name.
    """
    if sheet_names is None:
        sheet_names = [f"Sheet{i+1}" for i in range(len(dfs))]
    elif len(sheet_names) != len(dfs):
        raise ValueError("Length of sheet_names must match number of DataFrames.")
    
    with pd.ExcelWriter(file_name, engine='openpyxl') as writer:
        for df, name in zip(dfs, sheet_names):
            df.to_excel(writer, sheet_name=name, index=False)
    
    print(f"âœ… Excel file '{file_name}' created with {len(dfs)} sheets.")



