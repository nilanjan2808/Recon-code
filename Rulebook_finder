import os
import re
import pandas as pd

def create_mapping_table(df, col_name, folder_path, search_col):
    """
    Creates a mapping table showing which files in folder_path contain matches
    (case-insensitive, ignores underscores, spaces, and quotes) of unique values
    from df[col_name] within a specified column (search_col) in those files.
    Also captures corresponding target_system values.

    Args:
        df (pd.DataFrame): Input dataframe
        col_name (str): Column in df containing values to search for
        folder_path (str): Folder containing data files (.csv/.xlsx)
        search_col (str): Column name in the target files to search within

    Returns:
        pd.DataFrame: Mapping table with columns [col_name, 'files_found_in', 'target_system']
    """

    def clean_text(s):
        """Helper: clean string for flexible matching."""
        if pd.isna(s):
            return ""
        return re.sub(r"[\s_'\"]+", "", str(s).lower())

    unique_values = df[col_name].dropna().astype(str).unique()
    mapping_rows = []

    for file in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file)

        # Read only CSV or Excel
        if file.endswith('.csv'):
            try:
                temp_df = pd.read_csv(file_path, dtype=str, encoding='utf-8', errors='ignore')
            except Exception as e:
                print(f"⚠️ Error reading {file}: {e}")
                continue
        elif file.endswith(('.xlsx', '.xls')):
            try:
                temp_df = pd.read_excel(file_path, dtype=str)
            except Exception as e:
                print(f"⚠️ Error reading {file}: {e}")
                continue
        else:
            continue

        # Skip if target column not present
        if search_col not in temp_df.columns:
            continue

        # Clean the target column
        temp_df[search_col] = temp_df[search_col].apply(clean_text)

        # Handle target_system
        if 'target_system' in temp_df.columns:
            temp_df['target_system'] = temp_df['target_system'].astype(str).str.strip()
        else:
            temp_df['target_system'] = 'Not Available'

        # Search each unique value in the cleaned search_col
        for val in unique_values:
            cleaned_val = clean_text(val)

            # Regex partial match
            mask = temp_df[search_col].str.contains(cleaned_val, regex=True, na=False)

            if mask.any():
                targets = temp_df.loc[mask, 'target_system'].unique().tolist()
                mapping_rows.append({
                    col_name: val,
                    'files_found_in': file,
                    'target_system': ', '.join(targets)
                })

    # Mark values not found
    found_values = {row[col_name] for row in mapping_rows}
    for val in unique_values:
        if val not in found_values:
            mapping_rows.append({
                col_name: val,
                'files_found_in': 'Not Found',
                'target_system': 'Not Found'
            })

    # Combine multiple file hits per value
    result_df = (
        pd.DataFrame(mapping_rows)
        .groupby(col_name, as_index=False)
        .agg({
            'files_found_in': lambda x: ', '.join(sorted(set(x))),
            'target_system': lambda x: ', '.join(sorted(set(x)))
        })
    )

    return result_df
